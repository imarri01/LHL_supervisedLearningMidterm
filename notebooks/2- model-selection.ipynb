{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Imports***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Label Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label encoding was chosen for the `city` and `type` instead of One Hot Encoding to reduce the amount of columns being used in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../data/training/X_train.csv')\n",
    "y = pd.read_csv('../data/training/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['city_encoded'] = X['city'].astype('category').cat.codes\n",
    "X['type_encoded'] = X['type'].astype('category').cat.codes\n",
    "\n",
    "X = X.drop(columns=['city', 'type'])\n",
    "\n",
    "\n",
    "# Converting column datatypes into integers\n",
    "\n",
    "X['is_foreclosure'] = X['is_foreclosure'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Train Test Split for Validation Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3096, 29)\n",
      "(774, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting test/training data into CSV\n",
    "\n",
    "X_train.to_csv('../data/training/X_train_v3.csv', index=False)\n",
    "X_validate.to_csv('../data/training/X_validation.csv', index=False)\n",
    "y_train.to_csv('../data/training/y_train_v3.csv', index=False)\n",
    "y_validate.to_csv('../data/training/y_validatione.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Support Vector Machine': SVR(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'Ridge Regression': Ridge(random_state=42),\n",
    "    'Lasso Regression': Lasso(random_state=42),\n",
    "    'ElasticNet Regression': ElasticNet(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Performance:\n",
      "Mean Squared Error: 274501793270.35184\n",
      "Mean Absolute Error: 204038.20322228255\n",
      "R^2 Score: 0.3966762919721759\n",
      "\n",
      "Support Vector Machine Performance:\n",
      "Mean Squared Error: 483900366034.21\n",
      "Mean Absolute Error: 250867.79209560188\n",
      "R^2 Score: -0.06355794500856349\n",
      "\n",
      "Random Forest Performance:\n",
      "Mean Squared Error: 6612175921.268344\n",
      "Mean Absolute Error: 24107.056817715733\n",
      "R^2 Score: 0.9854671896768887\n",
      "\n",
      "XGBoost Performance:\n",
      "Mean Squared Error: 4439562534.9116125\n",
      "Mean Absolute Error: 21193.024010296016\n",
      "R^2 Score: 0.9902423466940842\n",
      "\n",
      "Decision Tree Performance:\n",
      "Mean Squared Error: 6193112904.076359\n",
      "Mean Absolute Error: 8933.185677371725\n",
      "R^2 Score: 0.9863882425065167\n",
      "\n",
      "K-Nearest Neighbors Performance:\n",
      "Mean Squared Error: 77581395348.8372\n",
      "Mean Absolute Error: 97467.70025839793\n",
      "R^2 Score: 0.8294849204510151\n",
      "\n",
      "Gradient Boosting Performance:\n",
      "Mean Squared Error: 34976229429.468475\n",
      "Mean Absolute Error: 109245.70462906508\n",
      "R^2 Score: 0.9231262274070615\n",
      "\n",
      "AdaBoost Performance:\n",
      "Mean Squared Error: 87353813837.12663\n",
      "Mean Absolute Error: 247798.41034593686\n",
      "R^2 Score: 0.8080062565468138\n",
      "\n",
      "Ridge Regression Performance:\n",
      "Mean Squared Error: 274532623656.19\n",
      "Mean Absolute Error: 203986.2675721868\n",
      "R^2 Score: 0.396608530292071\n",
      "\n",
      "Lasso Regression Performance:\n",
      "Mean Squared Error: 274502594826.23233\n",
      "Mean Absolute Error: 204035.60953179773\n",
      "R^2 Score: 0.3966745302435539\n",
      "\n",
      "ElasticNet Regression Performance:\n",
      "Mean Squared Error: 291398512534.5114\n",
      "Mean Absolute Error: 198455.72648782423\n",
      "R^2 Score: 0.3595392255854444\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f'{name} Performance:')\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Mean Absolute Error: {mae}')\n",
    "    print(f'R^2 Score: {r2}')\n",
    "    print('')\n",
    "\n",
    "\n",
    "# Evaluate each model\n",
    "\n",
    "for name, model in models.items():\n",
    "    evaluate_model(name, model, X_train, X_validate, y_train, y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Performance Comparison**\n",
    "\n",
    "Based on the performance metrics selected, the **Decision Tree Regressor** performed the best across all the models. See details results and comparison below, along with summary and next steps for Part 3 of the project:\n",
    "\n",
    "##### ***Best Performing Model: Decision Tree Regressor***\n",
    "- **Mean Squared Error (MSE)**: 6,193,112,904.08\n",
    "- **Mean Absolute Error (MAE)**: 8,933.19\n",
    "- **R² Score**: 0.9864\n",
    "\n",
    "##### ***Comparison with Other Models***\n",
    "\n",
    "| Model                         | Mean Squared Error (MSE)  | Mean Absolute Error (MAE)  | R² Score  |\n",
    "|-------------------------------|---------------------------|----------------------------|-----------|\n",
    "| **Decision Tree**             | 6,193,112,904.08          | 8,933.19                   | 0.9864    |\n",
    "| **Random Forest**             | 6,612,175,921.27          | 24,107.06                  | 0.9855    |\n",
    "| **XGBoost**                   | 4,439,562,534.91          | 21,193.02                  | 0.9902    |\n",
    "| **Linear Regression**         | 274,501,793,270.35        | 204,038.20                 | 0.3967    |\n",
    "| **Support Vector Machine**    | 483,900,366,034.21        | 250,867.79                 | -0.0636   |\n",
    "| **K-Nearest Neighbors**       | 77,604,134,366.93         | 97,416.02                  | 0.8294    |\n",
    "| **Gradient Boosting**         | 34,976,229,429.47         | 109,245.70                 | 0.9231    |\n",
    "| **AdaBoost**                  | 87,353,813,837.12         | 247,798.41                 | 0.8080    |\n",
    "| **Ridge Regression**          | 274,532,623,656.19        | 203,986.27                 | 0.3966    |\n",
    "| **Lasso Regression**          | 274,502,594,826.23        | 204,035.61                 | 0.3967    |\n",
    "| **ElasticNet Regression**     | 291,398,512,534.51        | 198,455.73                 | 0.3595    |\n",
    "\n",
    "##### ***Summary***\n",
    "\n",
    "The **Decision Tree Regressor** achieved the best performance in terms of Mean Squared Error, Mean Absolute Error, and R² Score, indicating that it is the most accurate model among those tested for this particular dataset.\n",
    "\n",
    "The **Random Forest Regressor** also performed very well, closely following the Decision Tree Regressor. The other models, especially the simpler linear models (Linear Regression, Ridge, Lasso, and ElasticNet), did not perform as well, which might suggest that the dataset benefits from more complex, non-linear models.\n",
    "\n",
    "Based on these results, you might consider focusing on the Decision Tree and Random Forest models for further tuning and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Next Steps for Part 3**\n",
    "\n",
    "Based on these results, we'll focusing on the Decision Tree and Random Forest models for further tuning and optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
